{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2\n",
      "  Using cached tensorflow-2.2.0-cp38-cp38-win_amd64.whl (459.2 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (1.15.0)\n",
      "Requirement already satisfied: gast==0.3.3 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (0.3.3)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (1.4.1)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (2.10.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (0.34.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (3.14.0)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (1.33.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.2) (0.11.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.23.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (49.2.0.post20200714)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in d:\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\n",
      "Installing collected packages: astunparse, keras-preprocessing, opt-einsum, tensorflow-estimator, google-pasta, tensorflow\n",
      "Successfully installed astunparse-1.6.3 google-pasta-0.2.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0 tensorflow-2.2.0 tensorflow-estimator-2.3.0\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in d:\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in d:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in d:\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in d:\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in d:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.2.2\n",
      "    Uninstalling tensorboard-2.2.2:\n",
      "      Successfully uninstalled tensorboard-2.2.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.2.0\n",
      "    Uninstalling tensorflow-estimator-2.2.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
      "Successfully installed tensorboard-2.4.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement libwebp (from versions: none)\n",
      "ERROR: No matching distribution found for libwebp\n"
     ]
    }
   ],
   "source": [
    "!pip install libwebp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비행기, 사과, 고양이 3가지의 이미지를 분류해본다\n",
    "# 이미지 수가 부족해 비슷한 데이터들을 각도를 다르게 해서 여러가지 데이터로 만든 후 진행했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane  파일 길이 :  150\n",
      "airplane  :  ./multi_img_data/imgs_others/train/airplane\\00000000.jpg\n",
      "apple  파일 길이 :  150\n",
      "apple  :  ./multi_img_data/imgs_others/train/apple\\a1r_327_100 (1).jpg\n",
      "cat  파일 길이 :  150\n",
      "cat  :  ./multi_img_data/imgs_others/train/cat\\cat.4001.jpg\n",
      "ok 450\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"./multi_img_data/imgs_others/train\"\n",
    "categories = [\"airplane\", \"apple\", \"cat\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    \n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#1 0 0 0 이면 airplanes\n",
    "#0 1 0 0 이면 buddha 이런식\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./multi_img_data/imgs_others/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane  파일 길이 :  150\n",
      "./multi_img_data/imgs_others/train/airplane\\00000000.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00000001.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00000002.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00000003.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00000004.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00000005.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00000006.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001036.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001037.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001038.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001039.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001040.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001042.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001043.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001044.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001045.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001047.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001048.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001049.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001050.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001052.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001053.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001054.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001056.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001057.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001058.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001059.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001060.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001061.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001062.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001063.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001064.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001065.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001066.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001067.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001068.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001069.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001070.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001071.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001072.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001073.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001074.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001075.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001076.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001077.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001078.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001079.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001080.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001081.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001082.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001084.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001087.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001089.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001090.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001091.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001092.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001093.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001094.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001095.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001096.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001097.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001098.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001099.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001100.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001101.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001102.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001103.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001104.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001105.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001106.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001107.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001109.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001110.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001111.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001112.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001113.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001114.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001115.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001116.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001117.JPG\n",
      "./multi_img_data/imgs_others/train/airplane\\00001118.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001120.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001121.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001122.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001123.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001124.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001125.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001127.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001128.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001129.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001131.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001134.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001135.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001136.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001137.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001138.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001139.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001141.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001142.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001143.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001144.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001145.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001146.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001147.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001149.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001150.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001151.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001153.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001155.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001156.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001157.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001158.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001160.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001161.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001162.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001163.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001164.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001165.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001166.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001167.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001168.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001169.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001170.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001171.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001172.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001173.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001174.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001175.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001176.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001177.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001178.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001179.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001180.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001181.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001182.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001183.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001184.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001185.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001187.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001188.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001189.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001190.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001192.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001193.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001194.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001195.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001196.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001198.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001199.jpg\n",
      "./multi_img_data/imgs_others/train/airplane\\00001200.jpg\n",
      "apple  파일 길이 :  150\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (1).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (10).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (100).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (101).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (102).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (103).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (104).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (105).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (106).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (107).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (108).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (109).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (11).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (110).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (111).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (112).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (113).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (114).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (115).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (116).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (117).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (118).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (119).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (12).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (120).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (121).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (122).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (123).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (124).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (125).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (126).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (127).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (128).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (129).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (13).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (130).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (131).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (132).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (133).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (134).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (135).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (136).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (137).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (138).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (139).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (14).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (140).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (141).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (142).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (143).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (144).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (145).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (146).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (147).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (148).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (149).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (15).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (150).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (16).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (17).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (18).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (19).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (2).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (20).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (21).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (22).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (23).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (24).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (25).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (26).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (27).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (28).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (29).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (3).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (30).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (31).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (32).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (33).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (34).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (35).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (36).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (37).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (38).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (39).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (4).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (40).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (41).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (42).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (43).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (44).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (45).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (46).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (47).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (48).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (49).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (5).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (50).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (51).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (52).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (53).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (54).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (55).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (56).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (57).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (58).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (59).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (6).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (60).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (61).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (62).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (63).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (64).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (65).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (66).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (67).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (68).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (69).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (7).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (70).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (71).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (72).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (73).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (74).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (75).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (76).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (77).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (78).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (79).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (8).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (80).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (81).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (82).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (83).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (84).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (85).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (86).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (87).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (88).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (89).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (9).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (90).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (91).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (92).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (93).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (94).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (95).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (96).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (97).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (98).jpg\n",
      "./multi_img_data/imgs_others/train/apple\\a1r_327_100 (99).jpg\n",
      "cat  파일 길이 :  150\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4001.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4002.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4003.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4004.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4005.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4006.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4007.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4008.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4009.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4010.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4011.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4012.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4013.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4014.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4015.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4016.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4017.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4018.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4019.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4020.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4021.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4022.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4023.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4024.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4025.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4026.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4027.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4028.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4029.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4030.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4031.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4032.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4033.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4034.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4035.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4036.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4037.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4038.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4039.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4040.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4041.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4042.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4043.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4044.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4045.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4046.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4047.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4048.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4049.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4050.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4051.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4052.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4053.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4054.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4055.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4056.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4057.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4058.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4059.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4060.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4061.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4062.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4063.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4064.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4065.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4066.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4067.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4068.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4069.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4070.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4071.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4072.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4073.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4074.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4075.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4076.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4077.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4078.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4079.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4080.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4081.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4082.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4083.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4084.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4085.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4086.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4087.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4088.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4089.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4090.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4091.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4092.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4093.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4094.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4095.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4096.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4097.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4098.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4099.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4100.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4101.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4102.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4103.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4104.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4105.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4106.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4107.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4108.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4109.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4110.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4111.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4112.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4113.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4114.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4115.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4116.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4117.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4118.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4119.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4120.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4121.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4122.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4123.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4124.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4125.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4126.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4127.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4128.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4129.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4130.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4131.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4132.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4133.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4134.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4135.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4136.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4137.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4138.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4139.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4140.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4141.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4142(1).jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4142.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4143.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4144.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4145.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4146.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4147.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4148.jpg\n",
      "./multi_img_data/imgs_others/train/cat\\cat.4149.jpg\n"
     ]
    }
   ],
   "source": [
    "for idx, cat in enumerate(categories):\n",
    "    #print(idx)\n",
    "    #print(cat)\n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 64, 64, 3)\n",
      "337\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "#import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./multi_img_data/imgs_others/multi_image_data.npy', allow_pickle = True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"airplane\", \"apple\", \"cat\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dir = './multi_img_data/imgs_others/model'\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 4,214,723\n",
      "Trainable params: 4,214,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9377\n",
      "Epoch 00001: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.1756 - accuracy: 0.9377 - val_loss: 0.2687 - val_accuracy: 0.8850\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9585\n",
      "Epoch 00002: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0861 - accuracy: 0.9585 - val_loss: 0.2008 - val_accuracy: 0.9115\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9852\n",
      "Epoch 00003: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 0.0469 - accuracy: 0.9852 - val_loss: 0.1887 - val_accuracy: 0.9115\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9941\n",
      "Epoch 00004: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 0.0328 - accuracy: 0.9941 - val_loss: 0.1957 - val_accuracy: 0.8938\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9911\n",
      "Epoch 00005: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 0.0250 - accuracy: 0.9911 - val_loss: 0.1911 - val_accuracy: 0.9381\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9881\n",
      "Epoch 00006: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 0.0360 - accuracy: 0.9881 - val_loss: 0.1911 - val_accuracy: 0.9027\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9941\n",
      "Epoch 00007: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.2108 - val_accuracy: 0.9292\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9941\n",
      "Epoch 00008: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.0257 - accuracy: 0.9941 - val_loss: 0.2363 - val_accuracy: 0.9204\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9911\n",
      "Epoch 00009: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.0331 - accuracy: 0.9911 - val_loss: 0.2222 - val_accuracy: 0.9204\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9970\n",
      "Epoch 00010: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 171ms/step - loss: 0.0196 - accuracy: 0.9970 - val_loss: 0.2581 - val_accuracy: 0.8938\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9941\n",
      "Epoch 00011: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.0172 - accuracy: 0.9941 - val_loss: 0.2109 - val_accuracy: 0.9292\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2177 - val_accuracy: 0.9292\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 173ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9115\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9941\n",
      "Epoch 00014: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.0121 - accuracy: 0.9941 - val_loss: 0.2824 - val_accuracy: 0.8938\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.9292\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.9115\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9911\n",
      "Epoch 00017: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0367 - accuracy: 0.9911 - val_loss: 0.2908 - val_accuracy: 0.8938\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9941\n",
      "Epoch 00018: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.2915 - val_accuracy: 0.9292\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9941\n",
      "Epoch 00019: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0149 - accuracy: 0.9941 - val_loss: 0.3060 - val_accuracy: 0.9292\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.8850\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9027\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3055 - val_accuracy: 0.9115\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.2835 - val_accuracy: 0.9292\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000    \n",
      "Epoch 00024: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.9115\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9970\n",
      "Epoch 00025: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0049 - accuracy: 0.9970 - val_loss: 0.2835 - val_accuracy: 0.9115\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9941\n",
      "Epoch 00026: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.0089 - accuracy: 0.9941 - val_loss: 0.3585 - val_accuracy: 0.9115\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 0.9115\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.9381\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9970\n",
      "Epoch 00029: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 0.0057 - accuracy: 0.9970 - val_loss: 0.3147 - val_accuracy: 0.9204\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9970 - ETA: 0s - loss: 0.0023 - accu\n",
      "Epoch 00030: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0051 - accuracy: 0.9970 - val_loss: 0.4153 - val_accuracy: 0.9204\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9970 ETA: 0s - loss: 0.0088 - accuracy\n",
      "Epoch 00031: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.3294 - val_accuracy: 0.9027\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9970\n",
      "Epoch 00032: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0059 - accuracy: 0.9970 - val_loss: 0.2900 - val_accuracy: 0.9204\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00033: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9027\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00034: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3021 - val_accuracy: 0.9204\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00035: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 159ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2985 - val_accuracy: 0.9381\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.3263e-04 - accuracy: 1.0000\n",
      "Epoch 00036: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 5.3263e-04 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.9381\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00037: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3145 - val_accuracy: 0.9381\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00038: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3175 - val_accuracy: 0.9115\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00039: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3120 - val_accuracy: 0.9292\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00040: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.9292\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00041: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4997 - val_accuracy: 0.8850\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9911\n",
      "Epoch 00042: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0295 - accuracy: 0.9911 - val_loss: 0.4096 - val_accuracy: 0.8938\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 00043: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.8938\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00044: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5649 - val_accuracy: 0.9027\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00045: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5546 - val_accuracy: 0.9027\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00046: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5786 - val_accuracy: 0.9027\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.8883e-04 - accuracy: 1.0000\n",
      "Epoch 00047: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 6.8883e-04 - accuracy: 1.0000 - val_loss: 0.5945 - val_accuracy: 0.9027\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00048: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5918 - val_accuracy: 0.9381\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000     ETA: 0s - loss: 6.0692e-04 - accuracy\n",
      "Epoch 00049: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.9292\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9970\n",
      "Epoch 00050: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.4886 - val_accuracy: 0.9027\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00051: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3749 - val_accuracy: 0.9204\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9911\n",
      "Epoch 00052: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0122 - accuracy: 0.9911 - val_loss: 0.6036 - val_accuracy: 0.8938\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9941\n",
      "Epoch 00053: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0162 - accuracy: 0.9941 - val_loss: 0.6150 - val_accuracy: 0.9027\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9941 ETA: 0s - loss: 0.0253 - ac\n",
      "Epoch 00054: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0194 - accuracy: 0.9941 - val_loss: 0.7756 - val_accuracy: 0.9115\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 00055: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.7188 - val_accuracy: 0.8761\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00056: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7594 - val_accuracy: 0.8938\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 9.5823e-04 - accuracy: 1.0000\n",
      "Epoch 00057: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 9.5823e-04 - accuracy: 1.0000 - val_loss: 0.7463 - val_accuracy: 0.9027\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00058: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7327 - val_accuracy: 0.9027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.4848e-04 - accuracy: 1.0000\n",
      "Epoch 00059: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 6.4848e-04 - accuracy: 1.0000 - val_loss: 0.7324 - val_accuracy: 0.9027\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00060: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7926 - val_accuracy: 0.9027\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3763e-04 - accuracy: 1.0000\n",
      "Epoch 00061: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 3.3763e-04 - accuracy: 1.0000 - val_loss: 0.7871 - val_accuracy: 0.9027\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.1307e-04 - accuracy: 1.0000\n",
      "Epoch 00062: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 4.1307e-04 - accuracy: 1.0000 - val_loss: 0.8030 - val_accuracy: 0.9115\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.5845e-04 - accuracy: 1.0000\n",
      "Epoch 00063: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 5.5845e-04 - accuracy: 1.0000 - val_loss: 0.8545 - val_accuracy: 0.9292\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0047 - accuracy: 0.9970 - val_loss: 0.9001 - val_accuracy: 0.9027\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00065: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.7904 - val_accuracy: 0.9292\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00066: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7377 - val_accuracy: 0.9115\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.2035e-04 - accuracy: 1.0000\n",
      "Epoch 00067: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 5.2035e-04 - accuracy: 1.0000 - val_loss: 0.7680 - val_accuracy: 0.8850\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.6823e-04 - accuracy: 1.0000\n",
      "Epoch 00068: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 6.6823e-04 - accuracy: 1.0000 - val_loss: 0.8037 - val_accuracy: 0.9115\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1092e-04 - accuracy: 1.0000\n",
      "Epoch 00069: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 1.1092e-04 - accuracy: 1.0000 - val_loss: 0.8555 - val_accuracy: 0.9292\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.1800e-04 - accuracy: 1.0000\n",
      "Epoch 00070: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 6.1800e-04 - accuracy: 1.0000 - val_loss: 0.7824 - val_accuracy: 0.9204\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.2931e-04 - accuracy: 1.0000\n",
      "Epoch 00071: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 3.2931e-04 - accuracy: 1.0000 - val_loss: 0.7661 - val_accuracy: 0.9292\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00072: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7226 - val_accuracy: 0.9027\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4381e-04 - accuracy: 1.0000\n",
      "Epoch 00073: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 3.4381e-04 - accuracy: 1.0000 - val_loss: 0.7040 - val_accuracy: 0.9115\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.5108e-04 - accuracy: 1.0000 ETA: 0s - loss: 8.0517e-05 - ac\n",
      "Epoch 00074: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 2.5108e-04 - accuracy: 1.0000 - val_loss: 0.6998 - val_accuracy: 0.9115\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.6834e-04 - accuracy: 1.0000\n",
      "Epoch 00075: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 3.6834e-04 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.9292\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.1836e-05 - accuracy: 1.0000\n",
      "Epoch 00076: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 8.1836e-05 - accuracy: 1.0000 - val_loss: 0.7111 - val_accuracy: 0.9292\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.7360e-04 - accuracy: 1.0000\n",
      "Epoch 00077: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 1.7360e-04 - accuracy: 1.0000 - val_loss: 0.7159 - val_accuracy: 0.9292\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.9453e-04 - accuracy: 1.0000\n",
      "Epoch 00078: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 6.9453e-04 - accuracy: 1.0000 - val_loss: 0.7234 - val_accuracy: 0.9115\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5479e-04 - accuracy: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 1.5479e-04 - accuracy: 1.0000 - val_loss: 0.7920 - val_accuracy: 0.9027\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.6706e-04 - accuracy: 1.0000\n",
      "Epoch 00080: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 2.6706e-04 - accuracy: 1.0000 - val_loss: 0.8051 - val_accuracy: 0.9027\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.7124e-04 - accuracy: 1.0000 ETA: 0s - loss: 1.5453e-04 - accu\n",
      "Epoch 00081: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 2.7124e-04 - accuracy: 1.0000 - val_loss: 0.8080 - val_accuracy: 0.9115\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.9973e-05 - accuracy: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 6.9973e-05 - accuracy: 1.0000 - val_loss: 0.8099 - val_accuracy: 0.9204\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5670e-04 - accuracy: 1.0000\n",
      "Epoch 00083: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 3.5670e-04 - accuracy: 1.0000 - val_loss: 0.8018 - val_accuracy: 0.9204\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0634e-04 - accuracy: 1.0000\n",
      "Epoch 00084: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 1.0634e-04 - accuracy: 1.0000 - val_loss: 0.7199 - val_accuracy: 0.9027\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.9386e-04 - accuracy: 1.0000 ETA: 0s - loss: 4.2511e-04 - ac\n",
      "Epoch 00085: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 1.9386e-04 - accuracy: 1.0000 - val_loss: 0.7054 - val_accuracy: 0.9115\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3759e-04 - accuracy: 1.0000\n",
      "Epoch 00086: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 1.3759e-04 - accuracy: 1.0000 - val_loss: 0.7017 - val_accuracy: 0.9204\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 7.4178e-05 - accuracy: 1.0000\n",
      "Epoch 00087: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 7.4178e-05 - accuracy: 1.0000 - val_loss: 0.6993 - val_accuracy: 0.9204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0631e-04 - accuracy: 1.0000\n",
      "Epoch 00088: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 1.0631e-04 - accuracy: 1.0000 - val_loss: 0.7002 - val_accuracy: 0.9204\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.8355e-05 - accuracy: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 4.8355e-05 - accuracy: 1.0000 - val_loss: 0.7036 - val_accuracy: 0.9204\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.4573e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 1.4573e-04 - accuracy: 1.0000 - val_loss: 0.7093 - val_accuracy: 0.9204\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.5238e-05 - accuracy: 1.0000\n",
      "Epoch 00091: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 5.5238e-05 - accuracy: 1.0000 - val_loss: 0.7160 - val_accuracy: 0.9204\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 7.2378e-05 - accuracy: 1.0000\n",
      "Epoch 00092: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 7.2378e-05 - accuracy: 1.0000 - val_loss: 0.7215 - val_accuracy: 0.9204\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2245e-04 - accuracy: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 1.2245e-04 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.9204\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.9003e-05 - accuracy: 1.0000 ETA: 0s - loss: 2.9140e-05 - accuracy: \n",
      "Epoch 00094: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 4.9003e-05 - accuracy: 1.0000 - val_loss: 0.7357 - val_accuracy: 0.9204\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5655e-05 - accuracy: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 3.5655e-05 - accuracy: 1.0000 - val_loss: 0.7365 - val_accuracy: 0.9204\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.8005e-05 - accuracy: 1.0000\n",
      "Epoch 00096: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 1.8005e-05 - accuracy: 1.0000 - val_loss: 0.7380 - val_accuracy: 0.9204\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.2110e-05 - accuracy: 1.0000\n",
      "Epoch 00097: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 6.2110e-05 - accuracy: 1.0000 - val_loss: 0.7400 - val_accuracy: 0.9204\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.5366e-05 - accuracy: 1.0000\n",
      "Epoch 00098: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 5.5366e-05 - accuracy: 1.0000 - val_loss: 0.7427 - val_accuracy: 0.9204\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.3235e-05 - accuracy: 1.0000\n",
      "Epoch 00099: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 4.3235e-05 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.9204\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 7.8697e-05 - accuracy: 1.0000\n",
      "Epoch 00100: val_loss did not improve from 0.12972\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 7.8697e-05 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.9204\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 30ms/step - loss: 0.7492 - accuracy: 0.9204\n",
      "정확도 : 0.9204\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXyU1dXHvzcLCTvIVgQhuCIggqAQUcuqgChqXVDEtyr6WqkF21rltSp1b3HBtlRrFZcWpRY3qigIJqAQZROVTY0CElEQlCWE7Of948zDTCYzYZLMZJvz/Xzm8yxzn/uc+0xyf889995znYhgGIZhxC8JtW2AYRiGUbuYEBiGYcQ5JgSGYRhxjgmBYRhGnGNCYBiGEeck1bYBlaVt27aSlpZWpWsPHDhA06ZNo2tQPSAeyx2PZYb4LHc8lhkqX+7Vq1fvEpF2ob6rd0KQlpbGqlWrqnRtZmYmgwcPjq5B9YB4LHc8lhnis9zxWGaofLmdc1vDfWeuIcMwjDjHhMAwDCPOMSEwDMOIc+pdH0EoioqKyMnJIT8/v8J0LVu2ZOPGjTVkVd2hpsudmppK586dSU5OrrF7GoZRdRqEEOTk5NC8eXPS0tJwzoVNt3//fpo3b16DltUNarLcIsLu3bvJycmhW7duNXJPwzCqR4NwDeXn59OmTZsKRcCoGZxztGnT5rCtM8Mw6g4NQggAE4E6hP0WdZisLHjgAd0aho8G4RoyDCMCsrJg6FAoLISUFFi8GNLTa9sqow7QYFoEhmEchsxMyM+H0lIVg8zM2rbIqCOYENQCzZo1i1peM2bMIC8vr8I0vXr1YteuXVG7p1FPOflk/36jRhCHs3GN0MSvEDQQX2kkQmAYAOzY4d9/5hlzCxmHaHh9BFOmwNq1Ib9qXFICiYmwdy988ok2kRMSoHdvaNkyfJ59+sCMGWG/vvXWW+natSs33ngjANOmTcM5x9KlS/nxxx8pKiri3nvvZezYsYc1/9tvv+Wyyy5j3759FBcX8/jjj3PmmWeycOFC7rrrLgoKCjjmmGN45plnmDVrFtu3b2fIkCG0bduWjIyMw+b/yCOPMGvWLAAmTpzIlClTOHDgAJdeeik5OTmUlJRwxx13cNlll3Hbbbcxb948kpKSOPvss3nooYcOm79Rh5k3D5wDEWjduratMeoQDU8IImHvXhUB0O3evRULwWEYN24cU6ZMOSQEL730Em+//TY333wzLVq0YNeuXQwcOJDzzz//sCNqXnjhBc455xxuv/12SkpKyMvLY9euXdx7770sWrSIpk2b8sc//pFHHnmEO++8k0ceeYSMjAzatm17WDtXr17NM888w4cffoiIMGDAAH7605/y1VdfceSRR/Lmm2/6Hs9efvjhB1599VU2bdqEc449e/ZU+fkYdYCDB2HhQjjvPBWEzZtr2yKjDtHwhKCCN/eD3sSqrCwYNkw7zBo1gtmzq9VM7tu3Lzt37mT79u18//33tG7dmo4dO3LzzTezdOlSEhIS+Oabb9ixYwc/+clPKszr1FNP5ZprrqGoqIgLLriAPn36sGTJEjZs2MCgQYMAKCwsJL0K9r7//vtceOGFh0LXXnTRRbz33nuMHDmS3/72t9x6662MGTOGM888k+LiYlJTU5k4cSLnnnsuY8aMqfyDMeoOixZBXh784hfw1lsmBEYZGp4QREJ6ug6dy8zUDrMo+Eovvvhi5s6dy3fffce4ceOYPXs233//PatXryY5OZm0tLSIJlmdddZZLF26lDfffJMJEyZwyy230Lp1a0aMGMGLL75YLRtFJOT5448/ntWrVzN//nymTp3K2WefzZ133smKFStYvHgxc+bM4a9//Svvvvtute5v1CKvvw4tWujw0a5dTQiMMsRvZ3F6OkydGrUOs3HjxjFnzhzmzp3LxRdfzN69e2nfvj3JyclkZGSwdWvYUOBl2Lp1K+3bt+e6667j2muvZc2aNQwcOJBly5aRnZ0NQF5eHp9//jkAzZs3Z//+/RHlfdZZZ/Haa6+Rl5fHgQMHePXVVznzzDPZvn07TZo04corr+S3v/0ta9asITc3l7179zJ69GhmzJjB2jD9LkY9oLQU/vtfGDVKW8DdusGWLbVtlVGHiM8WQQzo2bMn+/fvp1OnTnTs2JHx48dz3nnn0b9/f/r06UP37t0jyiczM5Pp06eTnJxMs2bNeP7552nXrh3PPvssl19+OQUFBQDce++9HH/88Vx//fWMGjWKjh07Hraz+JRTTuHnP/85p512GqCdxX379mXBggXccsstJCQkkJyczOOPP87+/fsZO3Ys+fn5iAiPPvpo9R6QUXs89RTs3AknnqjHaWnw2muVzycrK6qtaKMOISL16tOvXz8JZsOGDeXOhWLfvn0RpWto1Ea5I/1NYkVGRkat3r+2KFfu5ctFkpJEQKRxYz2+/3493r8/8oyXLxdJTRVxzp9PHcF+68gAVkmYejV+XUOGEQ9kZkJxse57s4m9qLARuisP5VNQoENPCwpsVnIDw1xDtcSnn37KhAkTypxLSUnhww8/rHKeAwYMOOQ68vjnP/9JWlpalfM06jm9eunWOf9sYm8I8+bN0LNnZPl414no3BubldygMCGoJU466aSod8CGE5FIO5ONBog3/+PGG2H8ePXtezOMKzNyqF8/DVR38CD07299BA0MEwLDaMgsXAht28Kf/6xv8gDt20PjxpUTgg8+UBFo395GHDVArI/AMBoqIjqRbPhwvwiAunjS0ipXob/zjoZnufFG+O47/RgNBhMCw2iorFunFfbZZ5f/rlu3yrUI3nkHTjvN3zfw0UdRMdGoG5gQGEZDZeFC3Y4YUf67ygjBjz/CypWaT58+es6EoEFhQhAF9uzZw9/+9rdKXzd69OiYB3Nbu3YtCxYsiOk9jDrKO+/oJLLOnct/l5amwRYj+ft7912dnTxihAZnPPpoE4IGRtwKQTSXIwgnBCUlJRVeN3/+fFq1alV9Aypg7dq1LPTeDI34IT8fliwJ3RoA/1yCSFoF77wDzZvDgAF63Ldv3RGCrCy6zJ6t/8gNZI2R2iCmo4accyOBx4BE4CkReTDo+5bAv4AuPlseEpFnqnPPCpYjoKSkcSyWI+C2227jyy+/pE+fPodCQ3Ts2JG1a9eyYcMGLrjgArZt20Z+fj6TJ0/m+uuvByAtLY1Vq1aRm5vLqFGjOOOMM1i+fDmdOnXi9ddfp3HjxiHv9+c//5knnniCpKQkevTowZw5czhw4AA33XQTn376KcXFxUybNo1Ro0Zx5513kpeXx4oVK5g6dSqXXXZZufx++OEHrrnmGr766iuaNGnCk08+Se/evVmyZAmTJ08GOLS+Qm5ubsj1Eow6xrJlKgah+gegrBD07Rs+n6wseOklXd0sOVnP9e0LL79c7fDtVcYLddGmDUyeTLeCAl1oB6CkRIe5ZmTYENdKEDMhcM4lAjOBEUAOsNI5N09ENgQkmwRsEJHznHPtgM+cc7NFpDBWdkHUlyPgwQcfZN26daxdu5bMzEzOPfdc1q1bRzffP9usWbM44ogjOHjwIKeeeio/+9nPaNOmTZk8vvjiC1588UX+8Y9/cOmll/Lyyy9z5ZVXhr3f5s2bSUlJOeRauu+++xg6dCizZs1iz549nHbaaQwfPpy7776b5cuX8+STT4a1/6677qJv37689tprvPvuu1x11VWsXbuWhx56iJkzZzJo0CByc3NJTU3lySefLLdeglEHeeYZfctJTQ39vTfJsKIWgbfYfX4+fPihHqen+4Xj44/hrLOiavZhycqCIUN0drMPByoAHgUFMGkS/OEP2mFusZEOSyxbBKcB2SLyFYBzbg4wFggUAgGaO12tpRnwA1BcnZtW9Oa+f/9BmjdvHu3lCMpx2mmnHRIB0Df4V199FYBt27bxxRdflBOCbt260cfXEdevXz+2VDC0r3fv3owfP54LLriACy64AICFCxcyb968Q6uI5efn8/XXX0dk7/vvv8/LL78MwNChQ9m9ezd79+5l0KBB/PrXv2b8+PFcdNFFdO7cOeR6CUYdY948/aMGXYhm8eLyf+CtW2tY6oqGkHphJUDfmDIzywrBRx/VvBD84x9lRICEBARwSUk6LLa4WLeffALnn69imJIS+hkYh4ilEHQCtgUc5wADgtL8FZgHbAeaA5eJSGlwRs6564HrATp06EBmUJyTli1bRjR7tqSkhP3799OrF8ybl8D77ydxxhnF9OpVSnUm3+bm5lJaWsr+/fvJy8sjJSXlkD3vvfceCxYsYOHChTRp0oTRo0fzww8/sH//fkSE3NxccnNzSU5OPnRNcXExBw4cCFumOXPmsGzZMubPn88f/vAHVqxYQUlJCc8//zzHHXdcmbRLlixBRCp8PiUlJeTm5h5K49k1adIkBg8ezMKFCxkwYADz5s2jb9++zJ8/nwULFjB+/Hh+9atfccUVV5TLMz8/v9zvVJPk5ubW6v1rihbr19Nq7Vr29OnDvp49yd+xg4Jf/IJG6JtyaUEBW2bN4uug0CMApzVvjrz+Op91786+EKEmWjRvjuc0Kk1K4uMWLdjne6ant27ND2+9xaaTT45Z2crYsn49nV59lXYZGTjnEOeQ5GSyJ02idNcuDvoi6nrPosM779Dp9dehtLTCZ1CfierfeLhodNX9AJeg/QLe8QTgL0FpLgYeRf9mjwU2Ay0qyrcuRh/dtWuXdOnSRUQ0IuC555576LvXXntNxowZIyIiGzdulJSUlENRA7t27Srff/+9bN68WXr27HnomunTp8tdd90V8l4lJSWyefNmEREpLCyU9u3by48//ihTp06VSZMmSWlpqYiIrFmzRkRE5s6dK5dffnmF9t90001y9913H7K/T58+IiKSnZ19KM3YsWPl1VdflS1btkhRUZGIiDz66KMyefLkkHla9NEaYNkykYQEjSTaqJHIX/4ieR076rmUFJHExPCRQpcv918bLs377+v3F11U/vuRI0V6945NuULZ2qiR2pKQIPLooxpB1WdTyN/as70ORkuNFtGMPhrLFkEOcFTAcWf0zT+Qq4EHfUZmO+c2A92BFTG0K+q0adOGQYMG0atXLxo3bkyHDh0OfTdy5EieeOIJevfuzQknnMDAgQOrda+SkhKuvPJK9u7di4hw880306pVK+644w6mTJlC7969ERHS0tJ44403GDJkCPfddx99+vQJ21k8bdo0rr76anr37k2TJk147rnnAJgxYwYZGRkkJibSo0cPRo0axZw5c8qtl2DUEq+95u/sKiyEm26iMai/889/ht27w/vHMzN15rF3ref2CeSpp6BZM3juOd0G0revjia65x6duRxLt0tmptoI6vY5eFAXlaqIQYOge3coKoJ//tPcQocjnEJU94O6nb4CugGNgI+BnkFpHgem+fY7AN8AbSvKty62COo6th5BA2XaNH3rTUzUj1btun///RVfG/iWnZJS/o15zx6RJk1Errsu9PX33ut/Q4/1G/fy5fpmH6b1Eva3/vnPRTp0iJ1dtUy9WI9ARIqBXwILgI3ASyKy3jl3g3PuBl+ye4DTnXOfAouBW0VkV6xsMow6wfLlcOed1R/vvmMHNGmio2P+9jdo3JjShAR/uOmKSE+H+fO1M/WSS8q/Mb/4oi52f911oa/3RouVlvpbFLGie3eVuHPOqVynb69e+ox2WZVyOGI6j0BE5gPzg849EbC/HQgz0NmYNGkSy5YtK3Nu8uTJXH311VXK75lnnuGxxx4rc27QoEHMnDmzyjYalcQb/lhYCNOn66zdqrotli1TF8jtt+vxSSexZdYsjr7mmsjyHDZMr9+4sbyN99wDxxyjIadDMWYMPPywf7GaQYOqVoZIWL1at7/9beWelbcWw/r18NOfRt+uBkSDCUMtIjhvwY0GQrQr6KuvvrrKIlIZxPM9G+XJzFS/NYT3zUfCnj3w6afws5/5z6Wn83VBAUdXJr8RI+Cuu7Q/oU2bsuP0k5M1/HSo/NLTddLWI4/A3Lkqau+/r9dG2x+/cqVu+/Wr3HWeEKxbZ0JwGBpEiInU1FR2795tFVAdQETYvXs3qeEmMsU7gwdrOGfQjs9gF06kYRI++EDfxM84o3r2DB+u+WRk6HFgx6w3dyAc6enwn//A1VfDG2/AHXdoKyPaIR5WrYJjj9W5D5XhyCN1pui6ddG1pwHSIFoEnTt3Jicnh++//77CdPn5+XFZQdV0uVNTU+kcKtCZoZXnkCE64qZjx7Jvz4sXw8iRWjE3alSxP/z991VQBgRPzakkp56qcYQWLYKLL4YePfT+gUtbHo5jj9VtYH9BNFsFq1bB6adX/jrntFVgQnBYGoQQJCcnl5nJG47MzEz6VhRXpYESr+Wus3jhEHJyYPt2fXMFHfIZvNB8uAp12TIdwtm0afVsSUpSYVq0SI/ff187kH/zG7jwwsgq9CFDNJ/i4sjFI1J27oSvv4Zf/apq1/fqpbGSPHGrz3gxlmIQMqNBuIYMo16xcycc5Ztis2SJ//zWrf7KqqRE/fSh3ERFRRr7J1odtMOHw5dfarTGv/8dLrsM/vSnyCub9HR49FHdv/326LcGQFsuVaFXL11P4dtvo2dTTeK5Ch99VAXg97+PifutQbQIDKNesXOnxgB66SUVgssv19bBxx/rcM0mTTRo3B/+EDpWzkcf6aSq6vYPeAwfrtsJE2D/fvjd7yqfxw03qL3r10fHJo9Vq1Qcq9qi9UJnrFvnb3nVRQLf9kXg1Vf1t3j6aX8r0SMG7jcTAsOoSUpKdFz7kUdqRe61CF56Sbe33ALHHaeulocfVr97QUHZf3xvSHG0WgTdu+sC9+vW6XKUVQkkmJQEY8dqOQoKVLyiwapVal/z5lW7PnDkULiQ3LEksIKH8vunnALbtmm0VG80WbhBL94gA8/9FsXYSSYEhlGT7N6tlXv79jqk8a23dNLTnDk6PNILGvizn8HMmRoCurRUr3vgAa0AXn9dR9Bs2aIdztXlgw/UfQLaKvHCTVeWiy7SN9jFi2H06OrbBSoEXoulKrRrp8862i2VighaL4GCAm3ZgT8kCISv8D0SEvTjDR6YMaNs2JAoTuIzITCMmmTnTt22bw9duuj+M8/oWPnp0/3p0tN1stm8efDss9o6AHWTeBXIsGHRCa8cGHeouLjqbodhwzS09csvR0cIvvlGfftV7R/wqMmRQ16M+/z8shV9uNUKndMZ0978ksREfzjtUJV/jDAhMIyaZMcO3XbooC2Apk3h3nv13KWXlk2bnq6fhAS4/349F1i5RMtXPHiwunK8BTqqOuonJUVnHM+dq+saDx1aPdv+9S9/vtWhVy9tqXhLEsYCrxXw3nvaf+ORkKAVe2AFH1zZ33mnfkK5kGooWJ4JgWHUJIEtguRkHbe/ciWcdJK/hRDMmDE6aqSwsHwlEo2hmunp2rKIRuXTsye88IJWbPfdV/UWS0aGP3TGlCn6fKpqV8+ecOAA3Hqruq+iXbkGruQG+vs4pwIW+EYPFVf2gXbVcLRUEwLDqEkChSAry7/A9qZN4X3zwRU1RP+N0Wt9VBevw7Oqk8vee0/DVixc6HenRKvl8/DD2u8SrdXK3ntPQ3UvXVpWBCZO1DWhQ/0+tVjZV4QJgWHUJDt36gib1q21cvMqu8ClIEMRXFHXoUqkDGefrW4sL0xFJC2WZct0zYDPPvN3gDqnLabS0uq3fLyIAyLlR2BVlqwsnXz3+ee6HKjnqgsc0XP11XX39wmDCYFh1CQ7duhIloSE6Pnm6xLeaJapU3Vo7MKFoVsvWVn6Zv7559oXEDyCJiEBrr1W3WXVbfkMHQqNG6vvvrRUO6G9EViR5Ov5/1NT1b3ktXo8EhN1/kc0bK0lTAgMoybZuVPdQhBd33xdIj1d35r79YNp0/yT4jx/uXPahxBcoSYkaKXqtQKuuio6z8R7zm+/rautzZzpb3H86U+6tkI4H36zZjq3I3jMvnPasou2rbWECYFh1CQ7d+qIIY9o+ebrGklJOoT0k0+0sjx4UGcfB7/5B1eosRou6T3n0lIdpSWiLbEpU/R7b3SPN7JIpOyY/8ByhRvXX48xITCMmmTHDv+ksYbO+efDY4/5F68JFIHaqlBHj9ZO48BQ28GVfvCY/0D/fwOq/AMxITCMmiTQNdTQCXR9tWmjb99ef0htVajhbKponH8DrfwDMSEwjJriwAH1Rwe6hho6ga6vk06qG/0h4WyCWp3UVZuYEBhGTeHNKo6XFkEwdbE/pKJhuXXN1hhi6xEYRk0ROJnMMOoQJgSGUVN4QhBPriGjXmBCYBhVIdJF5gOJd9eQUWexPgLDqCxekLHCwvKrh1WE1yJo1y629hlGJbEWgWFUlsxM/4IxXkC0SNi5E1q21FAFhlGHMCEwjMpy+un+/crECNqxw9xCRp3EhMAwKkuzZrpt2rRyIY3jaTKZUa8wITCMyrJmjW7z82HAgMivC44zZBh1BBMCw6gsq1frtqQE9uyJ/DprERh1FBMCw6gsXosA/IueHI7iYti1y4TAqJOYEBhGZSgq0tDKJ5+sx7t2RXbd7t0a5dJcQ0YdxITAMCrDhg0aVvmcc/Q40haBhZcw6jAmBIZRGbz+AU8IIm0R2Kxiow5jQmAYlWHNGmjeHAYO1ONIWwTLlul2+/bY2GUY1cCEwDAqw+rV0LcvNGmi8wgiEYLly+Gee3T/mmsqF5/IMGqAmAqBc26kc+4z51y2c+62MGkGO+fWOufWO+eWxNIew6gWxcXw8ce6KDtozKCKXENZWfCHP8CECf7lDysTksIwaoiYBZ1zziUCM4ERQA6w0jk3T0Q2BKRpBfwNGCkiXzvnzIFq1F02bdJF2D0haNs2fIvAC0yXn6/HgWv0RhqSwjBqiFhGHz0NyBaRrwCcc3OAscCGgDRXAK+IyNcAIrIzhvYYRuXJyvIvWfj553rulFN0265deCHIzNTRRQAJCTBxInTpEjdLHxr1i1gKQSdgW8BxDhA8H/94INk5lwk0Bx4TkeeDM3LOXQ9cD9ChQwcyq9i0zs3NrfK19Zl4LHc0ytxi/XpOvvlmEoqLKU1OZm+PHrRKSmLtokXs27GD7iUltNq2jQ9C3KdFixb0cQ4nQmlyMh/36sW+nj1VHGL4W9hvHT9EtdwiEpMPcAnwVMDxBOAvQWn+CnwANAXaAl8Ax1eUb79+/aSqZGRkVPna+kw8ljsqZZ4wQUQdOmU/jRuLLF8u8utfizRtGv76wYNF2rfXtDWE/dbxQ2XLDaySMPVqLFsEOcBRAcedgeCxcznALhE5ABxwzi0FTgY+j6FdhnF4PvwQ/v1vcE4/oOsPgL/Dt107OHBA+w0aNy6fR2IiHHusuYKMOk8sRw2tBI5zznVzzjUCxgHzgtK8DpzpnEtyzjVBXUcbY2iTYRyel17Sjt62beG//4V774XHH9fKPjHR3+Hbtq2mD9dPsHevLkRjGHWcmLUIRKTYOfdLYAGQCMwSkfXOuRt83z8hIhudc28DnwClqCtpXaxsMozDkpkJ48apE6i0FI44AqZO1e9OOsnfcZye7g8bsWuXdgQHs2cPHHdcDRluGFUnpmsWi8h8YH7QuSeCjqcD02Nph2FEzL33qgiABpjLzPS7dtLTy7p5Dtci2LMHWrWKmamGES1sZrERv2RlwQMP+Gf6fv01vPeeun8CXUDh8BahDzWpTMRcQ0a9IaYtAsOosyxeDGefrRV2aqoeP/aYjvl/8UX47LPDj/n3hCBUi+DgQW1RWIvAqAeYEBjxyX/+4x8FdPCghoJYsEBjAV10UWR5tGypLYdQQuCtXGZCYNQDzDVkxCfe27w3NHTBAt2++GLkQeESErSfIJRraO9e3ZpryKgHmBAY8UtCAtx9d9kWQGWDwoWLN2QtAqMeYUJgxCfZ2dCtG/z+9/Db35afIxAp4SKQmhAY9QjrIzDik+xsOOYY3U9P187iwDkCkdK2LawLMfXFXENGPcKEwIg/ROCLL2D8eP+54DkCkWItAqMBYK4hI/744Qd9Yz/22Orn1bYt7N7tX3jGw4TAqEeYEBjxR3a2bqMhBO3aaQvjhx/Knt+7F5KTdY6CYdRxTAiM+CPaQgDl3UNeeAlveKph1GFMCIz4IztbK+hu3aqfV7h4QxZnyKhHmBAY8ceXX8JRR0XHbVNRi8BGDBn1BBMCI/7Izo6OWwjCtwj27rUWgVFvMCEwGh5ZWXSZPTt8qIjAOQTVxVxDRgPAhMBoWGRlwbBhdHv6aRg2rLwY7N2rlXa0WgQpKdC8ubmGjHqNCYHRsMjMhPx8nEjouEFffqnbaAkBaD+BuYaMeowJgdGwGDxYYwaBboPjBkVz6KhHaiqsWOFvfRQWQl6eCYFRbzAhMBoW6ekwZIjuX3FF+bARnhBEq48gKws2bdJ8PVeUxRky6hkmBEbD48AB3ebmlv8uOxs6doSmTaNzr8xM/xrHnivKEwJrERj1BBMCo+GxdatuN20q/100h46Cup6SfLEbvRDWFmfIqGdEJATOucnOuRZOedo5t8Y5d3asjTOMSlNYCNu3IwkJGmE0OBhctIUgPR3uuEP3n3xSjz0hMNeQUU+ItEVwjYjsA84G2gFXAw/GzCrDqCo5OSDC3p49oaDA3zoAdRl9+y18803ky1FGwogRuvVaAOYaMuoZkQqBFzlrNPCMiHwccM4w6g6+iv/H/v31ONA99PLLul20KPQcg6qSllbm3uYaMuobkQrBaufcQlQIFjjnmgOlsTPLMKqIrzL+4dRT9TiUEJSWVn5t4opo314nlm3ZosfmGjLqGZGuUHYt0Af4SkTynHNHoO4hw6hbbN0KzpF77LEa/iFQCPbv16ijCQmVX5u4IhISoGtXf4tg7169T/Pm0cnfMGJMpEKQDqwVkQPOuSuBU4DHYmeWYVSRrVuhY0ckORlOOAE++0zPi8Cnn8LIkXDmmZVfm/hwdO1atkXQsqUKhGHUAyL9S30cyHPOnQz8DtgKPB8zqwyjqmzdqpUyQPfu/hbB+vUaD+iSS2Dq1OiKAGg/QWAfgfUPGPWISIWgWEQEGAs8JiKPAdbuNeoeW7aUFYKdO3UZSa8/wJt1HG26djYWAYwAAB/GSURBVNV75eWpa8j6B4x6RKRCsN85NxWYALzpnEsEkmNnlmFUgdJS2LatrBCAuocyMvS8N8In2nj5fv21tQiMekekQnAZUIDOJ/gO6ARMj5lVhlEVvv0Wior8QnDCCbrduBGWLIldawD8QrBliwmBUe+ISAh8lf9soKVzbgyQLyLWR2DULTwfvScE3bpBcjLMnQu7d8dWCLx7bt1qriGj3hFpiIlLgRXAJcClwIfOuYtjaZhhVJpgIUhKguOOg7ff1uNoDRcNRceOKjrWIjDqIZEOH70dOFVEdgI459oBi4C5sTLMqKdkZWnHbLSHZ0ZCoBB4C8V07w4bNsDRR0OXLrG7d2IiHHUUbN4M+/aZEBj1ikiFIMETAR+7scilRjDz5sHYsTp+PiUFFi+uWTHYuhWOOAKaNfOf88JN9+gR+/unpelcBRFzDRn1ikgr87edcwuccz93zv0ceBOYf7iLnHMjnXOfOeeynXO3VZDuVOdcibmb6jmxCuEQKYFzCEBbJ//+t+4vXBjdQHOh6NrVP4HNWgRGPSLSzuJbgCeB3sDJwJMicmtF1/iGmM4ERgE9gMudc+Vey3zp/ggsqJzpRp2jdWvdOhfdEA6REiwEmZn+MNQlJbEXprQ0//1MCIx6RKSuIUTkZeDlSuR9GpAtIl8BOOfmoBPSNgSlu8mX76mVyNuoi3iV4JFHwn/+U7NuIREVgrMDlskYPFgFqbCwZoQpUITMNWTUIyoUAufcfkBCfQWIiLSo4PJOwLaA4xxgQFD+nYALgaFUIATOueuB6wE6dOhAZhXf7HJzc6t8bX2mpsp90ooVtAFKdu/mvfz8GnUNJe3dyxkHDpBdVEROZqaWGWgxfTqt1q5lT58+7CsoiKlNLX/8kb6+/VXZ2eQmJsbsXuGIx7/xeCwzRLncIhKTDzrU9KmA4wnAX4LS/AcY6Nt/Frj4cPn269dPqkpGRkaVr63P1Fi5jztOxDkRENm6tWbu6bF6td73lVdEpJZ+682b1QYQyc6u+ftLfP6Nx2OZRSpfbmCVhKlXYznyJwc4KuC4M7A9KE1/YI5zbgtwMfA359wFMbTJiBXFxTp08vTT9Xjjxpq9/8KFuvVWB6sNOnfWYaRgriGjXhFLIVgJHOec6+acawSMA+YFJhCRbiKSJiJp6JyEG0XktRjaZMSKbdtUDM47T483BHcFxZCsLLjzTt2/8cbYjw4KR1ISdOqk+yYERj0iZkIgIsXAL9HRQBuBl0RkvXPuBufcDbG6r1FLZGfrduBAXRCmJoUgM1NjDEHtDFsN5IgjdIbxqlW1Z4NhVJKIRw1VBRGZT9B8AxF5Ikzan8fSFiPGeEJw7LE6easmhWDwYB2yKuIfHVRQUHP398jK0gllJSW6JnJNT6gzjCpis4ON6PDll5CaqjF3evTQPgIJNeAsBpx8sm6HDq3dyjewJVLbLRPDqAQxbREY9ZzAuEEQet+rdLOz4ZhjNLxEjx7w44+wYwf85Cext/OTT1R0fvWr2n0Dr+l5C4YRJUwIjNBkZekbdkGBf+3d0tKy+6mp/jfwL79UtxDAiSfqdsOGmhGCjz7Sbd++FaeLNenp+jxqK+ieYVQRcw0ZocnMVBEQUZ93SUn5fW+CVmmpCsExx+i1XoC3mhpCumYNtGmj0T9rm/T02KyJbBgxxFoERmiCO2Cd0+GhiYm6X1Cg28GDdWWwgwf9LYKOHXX4ZE11GH/0kbYGnKuZ+xlGA8NaBEZoBg7UcM79++tbf0YG3HOPf79/fw01fcop2hoAvxA4V3Mjh4qKdKTOKafE/l6G0UCxFoERmu3bdYGV//kfv5sj0N0xbRqMGQPvvqstAvC7hkD7Cd54I/Z2btignbO13T9gGPUYaxEYoVm7Vrd9+oT+ftgwaN4cXnlFWwRJSWVXAOvRA3bu1LWCY8maNbq1FoFhVBkTAiM0nhD07h36+9RUGD0aXn9dF2NJS1Mx8PA6jP/v/2Ib8mHNGnVheW4pwzAqjQmBEZq1a3Wd3xYVRBq/8EJdG3j+/PIVcX6+bv/xD209xEoMPvpIWy0J9qdsGFXF/nuM0Hz8cXi3kMfo0TqiKHDEkMemTboVie4s26wseOAB3ZaUqGBZ/4BhVAvrLDbKs3+/zhSeMKHidM2bw4gR8OabGoI6K8vfoTx4sAZfKyrSbXVm2WZlwaJFkJcHDz+sw1hTUnS8/oEDFbdaDMM4LNYiMMrz6af6Jn+4FgH4+xDeequsCyg93b9w/JVXVn2CVVYWnHWWhpl+8EEVFhF1Pd11l6Z56KHaCz1tGA0AE4KGwvLlfpdJOALdKhXhdRR7wdwqonFjnTdQWlreBXThhVqJv/de1QPQPfGEtgBA75OUpJPaAvsEiostwJthVANzDTUEZs/2u3EC4/8E4r1Zl5SET+Px8cfQunVkIRuGD1dxCRdobfx4+N//1dE9/fr5bfHi8ezeDStXwsiR+l1gnJ7vvoN581QAEhI0/xkz9Jo2bWDKFAvwZhhRwISgvvP553DDDf43bu+tPLiSX7zY/2btxQjy0gRWzOnp2iLo0yeykA2HC7R28cXwy1+qWPXrp/caMkTtDGwl3HOPVvYi6v9/9FHtD8jLg3/9C7ZuLZ//SSdZgDfDiAImBPWNwEr7m2/g2mv1fFKSVvRJSaHfjtu39++LwKmn6v6SJerb91oKCxdqWOdf/CJym9LTw1fERxwBo0bB88+rsLzySuhFY7yAdqCjkG7wLWKXnAzdusEVV1TuvoZhRIz1EdQnMjPhzDN1ktbpp8Mll2gYiKIidZmkpsIZZ4SuHHft0u2ECVrpzpunb9vXXeevgPPz4amndBtJ/0CknHqqunMeeQS2bFEff2KiunRSUsruJySU9f+Xlpr/3zBijLUI6grB7plQPPusv9IOpLhYBeH667VzddcuXTc4kIwMdaU8/7xGBv3LX2DuXI0TlJysFW5JCfzzn5o+mpE8S0v9+4mJKj5duoRf8Mb8/4ZRo5gQ1AU8v3lBgY7CCdeR683W9d6ovdDQXmXZrBn8+c/qU58y5dBlrrAQli3TChhg7FiYOVNFoFEjFYXdu7XinTZN09xwAxx3XHRcLyNG6NBPr2K/6qqy+YbaN/+/YdQYJgR1gcWL/X7z4I7cQDZs0PDPF10UfsnIAQPUvTN58qG3+habNqnffcgQTbNypX+tgZISFYGpU3X0T2KingvX6VwVqrJyl/n/DaPGiB8hyMqiy+zZ6oeuaxXMN9/49xMTQ7tCvvtOJ3o9+CDceqv/fHBZJk7UN/8bbzz05t3qo4+04v/pTzXN4MH6HIJdL7Fcc9cqdsOos8SHEGRlweDBdCssVLfJu+/WnUppyxZ47jkd4//hhxq/J5Rt776r22HDKs4vLU23Tzyh+S5eTCtvOGjr1vpduDd0W3PXMOKS+BCCzEwoLsZBdF0eFRFJ529Wlg7/FFGBGjcufPz+RYu0Ij9cgLVAt09BASxaRMv16+Gmm8qmC/eGbm/uhhF3xIcQ+FwhcvAgLiEh9qNQvM7foiJ1wYSb6et1ECcnQ04O9Oyp4+xFyo7aEVEhGDpUXUcVMXiwDiM9eFBH6+zbR0JRkb9/wDAMI4j4mEfgc3kUtm6ts1tj/cabkaEVfKj4Ox6Zmf4OYm+sfM+e2iLYubNs2uxs2LZNwzkcDs+9M3WqRuV8+GHEORUHwzCMEMSHEACkp/PDwIFaqVY1AFqkdOjg3w8303fAAN065++Y7dlTz61bVzbtokW6jUQIQMXg/vtVDET0c/75FqHTMIyQxI8QAPu7d9c37i1bYnujFSu0ck9I0Fg7Fc30nTjR7zryhGD9+rJpFy2Crl3LLg4fCT4XU5m+EcMwjCDiSgj2nXCC7qxcGb1Mg0M75+bCCy9obJyhQ3UpxVC89BL85Cfw+ON+ofjJT7RDOFAISko0/k+7dvDBB5WzzddfUOpF7rQZuoZhhCCuhODA0UdrhRgtIcjK0rH5t9/uX5TlP/9RMZg4EcaM0UlgX31V9rrcXF3V6+KLy3b+Oge9epUVgqef1vRr1lR+7V9ff8GWa66pOOy0YRhxTVwJgSQn63j6aAnBCy+UXTErM1Nn9XbvrkHhxozRdG+8Ufa6N97Q9JdeWj7Pnj1VCLx+jGee0W1FHc8VkZ7O1+PHmwgYhhGWuBICQCNhrl4dOnhbZRAp66oRgbff1pXChg/Xt/tjjlFRCBaCl16Cjh1h0KDy+fbsCXv2wPbtWvFv2FA2Wqe5dwzDiDLxJwT9+6ur5fPPq5fPggWwapXG9Ln3Xs136VL97qmn/C6cMWP0LX7/fj1etEhDQA8aVDbcskdgh/GCBRpV9MEHdeEWc+8YhhED4mNCWSDegiwrV8KJJ1Ytj5IS+N3v4Oij4U9/0jd10JaGiLqLvNnLY8bo4urvvKMdvqNG6fX//a+KRXDFHigEWVl6zeTJOunMMAwjBsRfi6B7d2jatHr9BHfeqQHgrr7aLwJDh+qkrWAXzumna3joX/1K4wh5y0WGW3C9fXtdS2D5cm05jBtnImAYRkyJqRA450Y65z5zzmU7524L8f1459wnvs9y51wUl8UKQ2Kizi6uqhC89ZZO1gLdei4gb0ZvsAtn1SoN9/DNN3DggFbqh/P39+wJL7+sM4+9RekNwzBiRMxcQ865RGAmMALIAVY65+aJyIaAZJuBn4rIj865UcCTwIBY2XSITp20w3bpUo36GSki8Jvf+I+DA9iFCtgW+NafkKBB5rzVucL5+9u21Xt16aJ9D4ZhGDEkli2C04BsEflKRAqBOcDYwAQislxEfvQdfgB0jqE9SlaWvm2XlOjKWcHj8oMniAXy1FOwcWNkb/UeXox/L/1VV2noh4oiks6bp/vfflv5SWSGYRiVJJadxZ2AbQHHOVT8tn8t8FaoL5xz1wPXA3To0IHMKoZKyM3N5avZs+nmC0kthYX8cPPNbL3ySlqtXUvKd99x5Pz5IIIkJ/PFL39J8v797OnTh5QdOzjxgQfIPeEEsidNotUnn7CnTx/2eSuKVUCL6dNptXZtROm7zJ5Nt5ISHFBaUsKWWbP42gtOV0Vyc3Or/MzqK/FYZojPcsdjmSHK5RaRmHyAS4CnAo4nAH8Jk3YIsBFoc7h8+/XrJ1UlIyNDZPlykcaNRRIT9QP+bSSf1FTNI1YE2te4cVTulZGRUX276hnxWGaR+Cx3PJZZpPLlBlZJmHo1lq6hHOCogOPOwPbgRM653sBTwFgRCbMqSxQJ7NRdskT7CLzJZc753T7h4v57Q0Nrwj6bN2AYRg0QS9fQSuA451w34BtgHHBFYALnXBfgFWCCiFRzhlfFZGXB7NldfEsWB3TqPvigDv0sKlIf/owZGqG0TRuYMkU7hBMTVSSKi2tmdq+tEmYYRg0SMyEQkWLn3C+BBUAiMEtE1jvnbvB9/wRwJ9AG+JvTFbmKRSTqw2SysrSuLyjoxuzZQS/a6em6HnCoZSVPOsl/HmwtX8MwGiQxnVksIvOB+UHnngjYnwhMjKUNoPV3YSGIuNBLFke6fq8JgGEYDZC4mFnsjeAE9fJY3DbDMAw/cSEE6ekaGNQ5wSIyG4ZhlCUuhAB0/ZhOnQ6Sm1vblhiGYdQt4kYIALp0yWPjxtq2wjAMo24RV0Jw1FF5fP559dekMQzDaEjElRB06ZJHYSFs2VLblhiGYdQd4k4IADZtqmVDDMMw6hBxJQRHHWVCYBiGEUxcCUHLlsW0a2dCYBiGEUhcCQHoSpUmBIZhGH5MCAzDMOKcuBSCXbv0YxiGYcSpEAB89lnt2mEYhlFXiFshMPeQYRiGEndC0LUrpKSYEBiGYXjEnRAkJsLxx5sQGIZheMSdEICNHDIMwwgkLoWgWTP48ktdu94wDCPeiTsh0EXsQQRGjtRjwzCMeCbuhCAzE4qLdb+gQI8NwzDimbgTgsGDddRQ4LFhGEY8E3dCkJ4OixfDeeepe6hp09q2yDAMo3aJOyEAFYOnn9ahpLNn17Y1hmEYtUtcCgFAu3baWfzCC1BaWtvWGIZh1B5xKwQA48dDTg4sXVrblhiGYdQecS0EY8fqnIJ//au2LTEMw6g9kmrbgNqkSRO48EKYM0djEA0frv0HhmEY8URctwgATjkFDhyAu+6CYcNsgplhGPFH3AtBXh44p0NJ8/PDTzDLyoIHHqh9oagrdhiG0XCIa9cQwJAhkJqqIiACH3ygFe3gwX430YIFcO65+n1Kis5DCOVCev99jV80dGhsXExvvgnnn6/7Fdlh+MnKUnEP/D0NwyhL3AuBN8EsIwP++1+YN08/qanw2GPw6afw3HNQUqLpCwu1YvEqlawsvX7TJh2KKqLXvvtu9CueRx7xD3UNtsMoT1aWuvsKCkw4DaMi4l4IQCuH9HStxD/80O8muuEG3QdIToaiIhWEE0/Uc++9px3MhYVl88vPh9dei26lk58Pa9b43Vgi8NOfRi//hkhmpr+l58WVMiEwjPKYEAQwdCjcd59W7KWlfhFITIRrr9U3/SefhN/9Dv7xD3UDeSLgHCQlqVCUlur3hYVw6aX6fXXdE7Nnw5498Je/wFtvwfz52nJZssTcHuE49VT/bygCAwfWni2BLioRbTEOG2a/m1E3MCEIwHMTZWZCmzYwebK2Aho1gquu0u+POgp+8xv44gut/JOTteJv1AhmzIDdu3UU0n336fGMGZoOVEjCuScq8mWLwKOPwsknw6RJcOONMGAA/PGPfhvuvFP3hwyxysXDW29i9GgVzuXL9fnUFN5vesQR+rfkvTR44nTPPdZKMeoGJgRBeG4igJNOKl85FxRAQoJW/gkJ2lLo0qVsmgce8KcB/z/+wYPwf/8HI0b4K6QFC7SCePhhFZ2UFO2b2L3bn+c778D69fDss1rZO6cuqVWrNO/CQvj97zW/aPZPVLejtTY7anfvbsQjj8Bll+k8kQsvhAcfhIkToUOH6N4rsJyg+02bwi23lHcbBlJYCDNnmhAYtU9MhcA5NxJ4DEgEnhKRB4O+d77vRwN5wM9FZE0sbaoMgaLg4YWxLiws21IIlyYxUSvu4mIVhszMitdA8PomvDf9V16Bu+/WGdBpaf5055+vguFVNJ4rKz9fRWHYML/YzJ7d5VDo7eAKK7iSzsqChQvh++/h739Xu5OTYfp0HWob6tpFi2DZMhWn0lLtNP/2W+18F1GX2W236TMZOjQyOypLcGX8+9/3oqBAW2agraeePeF//1dbU8H3jsSOUBV+o0Ywdao+J6/lFyp2VWKif+v9PYjAiy9q+ptu0vvGWjxDlcFci4YT73U12hk7lwh8DowAcoCVwOUisiEgzWjgJlQIBgCPiciAivLt37+/rFq1qko2ZWZmMjgKCxBE8s8a6h9u61btOwiuKJzTCsLrBA5VkThX3rXk3aNNG5gypXzfhnediJCYqLVUYN4iet8rrtC+je+/1/wrCsLnVWhei6hFC/jxx/DpQ5UjMdF/vXN670aNYNo0FZvhw1U8MjPhzDNV3N59V11jeXnaod+3r+a3Zg00bgyPP162Mi4pEZKSHEuX+p/XJZfA3LnBz8Zvh+fiu+MOveewYXr81lua/k9/8t/Dq8wrwntWgW7D4FbDzTfrfZ2Dfv3g44/1eSQna8ty927o319bi2vW6LNp3Fh/p5NPhv37Ye1adX+lpsKsWV9xzTVHH7qHd79XXtG4Wi+9VPbZl5bqva67Tu87fLj+PWVlHV4sQ/2Np6era3TFCjjnHL1HqOujuR+qzOHsq4v7VbE1Pb3y9ZlzbrWI9A/5pYjE5AOkAwsCjqcCU4PS/B0VB+/4M6BjRfn269dPqkpGRkaVr40Gy5eLNG4skpgo0qiRSEqK7jduLPL3v4vcf79uA9OcfrqIcyoRiYmaJlze998vcv31/vTV+TgnkpSk90xMDJ+uQ4fQ90tIEElO1muTkqJjU2U/wc/rjjtiY4f3nML9psuXh/7N7r+/7LNNTY2GPaU18mybNIleXtX/TUrL5OOcSMuWlb9HYJqa2neu/P9QRfvO6d/W8uWVr8+AVSKh69VYuoY6AdsCjnPQt/7DpekEfBuYyDl3PXA9QIcOHcis4vqSubm5Vb42Wkyf3oK1a1vRp88egEP7xx+/L2ya1atPpqjIkZQktGjxMZmZ+0LmnZ4OLVq0oFEjTZ+YKICjpISw+/pW6BBxOFdKQoL+2SUnC5MmZbNvXzItWhQxc+ax5fJMThbGj88O+13F1+v9nPP2hdJSBzhAfCWKbN85wTnNNyHBy5Nyz6tDh8M9G803nB0JCeXvEVjOcL9puCVRW7RoQVLSyYg4kpOFG2/M5q9/PZbi4oQK7ajqvnNaBpHwfwOR5tmsWQF5eSlRsU+k+mULzEdESE4uAFLKfRepHTW1LyIUFhYi0ijiawoKSpk1awtjx0avPoula+gS4BwRmeg7ngCcJiI3BaR5E3hARN73HS8Gficiq8PlWxdcQzVNZf3Gwc3McO6CYLdSsAsjuO+gss3aSK4PZUegHz2S/VCuF6/Mwc+rqnaEc+9Es28j8FlW9XkUFZWSlJQQ8XOqyr28fKr6e0V7P7jMdc2+aD/LRo3UNVhQED3XUCyFIB2YJiLn+I6nAojIAwFp/g5kisiLvuPPgMEi8m2ILIH4FILqcrhy15UwDNXx6QbbXZ3fuq50qFbleVQk+pXt04q2XztW+9ZHEBm11UeQBHwFdAMaAR8DPYPSnAu8hbZ9BgIrDpdvfe4jqC3isdzxWGaR+Cx3PJZZpJ70EYhIsXPul8ACdPjoLBFZ75y7wff9E8B8dMRQNjp89OpY2WMYhmGEJqbzCERkPlrZB557ImBfgEmxtMEwDMOomITaNsAwDMOoXUwIDMMw4hwTAsMwjDjHhMAwDCPOidk8gljhnPse2FrFy9sCu6JoTn0hHssdj2WG+Cx3PJYZKl/uriLSLtQX9U4IqoNzbpWEm1DRgInHcsdjmSE+yx2PZYbolttcQ4ZhGHGOCYFhGEacE29C8GRtG1BLxGO547HMEJ/ljscyQxTLHVd9BIZhGEZ54q1FYBiGYQRhQmAYhhHnxI0QOOdGOuc+c85lO+duq217YoFz7ijnXIZzbqNzbr1zbrLv/BHOuXecc1/4tq1r29Zo45xLdM595Jx7w3ccD2Vu5Zyb65zb5PvN0+Ok3Df7/r7XOededM6lNrRyO+dmOed2OufWBZwLW0bn3FRf3faZc+6cyt4vLoTAOZcIzARGAT2Ay51zPWrXqphQDPxGRE5E13eY5CvnbcBiETkOWOw7bmhMBjYGHMdDmR8D3haR7sDJaPkbdLmdc52AXwH9RaQXGuJ+HA2v3M8CI4POhSyj7398HNDTd83ffHVexMSFEACnAdki8pWIFAJzgLG1bFPUEZFvRWSNb38/WjF0Qsv6nC/Zc8AFtWNhbHDOdUYXOXoq4HRDL3ML4CzgaQARKRSRPTTwcvtIAho755KAJsB2Gli5RWQp8EPQ6XBlHAvMEZECEdmMru9yWmXuFy9C0AnYFnCc4zvXYHHOpQF9gQ+BDuJb/tO3bV97lsWEGcDvgNKAcw29zEcD3wPP+FxiTznnmtLAyy0i3wAPAV8D3wJ7RWQhDbzcPsKVsdr1W7wIgQtxrsGOm3XONQNeBqaIyL7atieWOOfGADtFZHVt21LDJAGnAI+LSF/gAPXfHXJYfH7xsegSuEcCTZ1zV9auVbVOteu3eBGCHOCogOPOaHOyweGcS0ZFYLaIvOI7vcM519H3fUdgZ23ZFwMGAec757agLr+hzrl/0bDLDPo3nSMiH/qO56LC0NDLPRzYLCLfi0gR8ApwOg2/3BC+jNWu3+JFCFYCxznnujnnGqEdK/Nq2aao45xzqM94o4g8EvDVPOB/fPv/A7xe07bFChGZKiKdRSQN/V3fFZEracBlBhCR74BtzrkTfKeGARto4OVGXUIDnXNNfH/vw9C+sIZebghfxnnAOOdcinOuG3AcsKJSOYdb1b6hfYDRwOfAl8DttW1PjMp4Btok/ARY6/uMBtqgowy+8G2PqG1bY1T+wcAbvv0GX2agD7DK93u/BrSOk3L/AdgErAP+CaQ0tHIDL6J9IEXoG/+1FZURuN1Xt30GjKrs/SzEhGEYRpwTL64hwzAMIwwmBIZhGHGOCYFhGEacY0JgGIYR55gQGIZhxDkmBIYRY5xzg72oqIZRFzEhMAzDiHNMCAzDh3PuSufcCufcWufc331rHOQ65x52zq1xzi12zrXzpe3jnPvAOfeJc+5VLza8c+5Y59wi59zHvmuO8WXfLGDtgNm+WbE45x50zm3w5fNQLRXdiHNMCAwDcM6dCFwGDBKRPkAJMB5oCqwRkVOAJcBdvkueB24Vkd7ApwHnZwMzReRkNAbOt77zfYEp6HoYRwODnHNHABcCPX353BvbUhpGaEwIDEMZBvQDVjrn1vqOj0ZDW//bl+ZfwBnOuZZAKxFZ4jv/HHCWc6450ElEXgUQkXwRyfOlWSEiOSJSiob+SAP2AfnAU865iwAvrWHUKCYEhqE44DkR6eP7nCAi00KkqygmS6hwwB4FAfslQJKIFKMLiLyMLjLydiVtNoyoYEJgGMpi4GLnXHs4tD5sV/R/5GJfmiuA90VkL/Cjc+5M3/kJwBLRtR9ynHMX+PJIcc41CXdD37oRLUVkPuo26hOLghnG4UiqbQMMoy4gIhucc78HFjrnEtCoj5PQBV96OudWA3vRfgTQMMBP+Cr6r4CrfecnAH93zt3ty+OSCm7bHHjdOZeKtiZujnKxDCMiLPqoYVSAcy5XRJrVth2GEUvMNWQYhhHnWIvAMAwjzrEWgWEYRpxjQmAYhhHnmBAYhmHEOSYEhmEYcY4JgWEYRpzz/39y2/AUXOOLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002352EDEEB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\고양이 (2).jpg이미지는 고양이로 추정됩니다.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\고양이 (3).jpg이미지는 고양이로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\고양이 (4).jpg이미지는 비행기로 추정됩니다.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\고양이.jpg이미지는 고양이로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\고양이.png이미지는 비행기로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\비행기 (2).jpg이미지는 비행기로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\비행기 (3).jpg이미지는 비행기로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\비행기 (4).jpg이미지는 비행기로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\비행기 (5).jpg이미지는 비행기로 추정됩니다.\n",
      "[1.000 0.000 0.000]\n",
      "0\n",
      "해당 test\\비행기.jpg이미지는 비행기로 추정됩니다.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\사과 (2).jpg이미지는 고양이로 추정됩니다.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\사과 (3).jpg이미지는 고양이로 추정됩니다.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\사과 (4).jpg이미지는 고양이로 추정됩니다.\n",
      "[0.000 0.000 1.000]\n",
      "2\n",
      "해당 test\\사과 (5).jpg이미지는 고양이로 추정됩니다.\n",
      "[0.000 1.000 0.000]\n",
      "1\n",
      "해당 test\\사과.jpg이미지는 사과으로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "caltech_dir = \"./multi_img_data/imgs_others/test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "model = load_model('./multi_img_data/imgs_others/model/multi_img_classification.model')\n",
    "\n",
    "prediction = model.predict(X)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"비행기\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"사과\"\n",
    "    else: pre_ans_str = \"고양이\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"/\")[-1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[-1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[-1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
